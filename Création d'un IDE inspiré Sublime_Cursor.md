# **Architecture et Ingénierie d'un Environnement de Développement Intégré (IDE) Natif IA : Synthèse des Modèles de Performance et d'Intelligence Contextuelle**

L'évolution de l'ingénierie logicielle traverse actuellement une transition paradigmatique fondamentale, redéfinissant la nature même des outils utilisés par les développeurs. Les environnements de développement intégrés (IDE) ne sont plus considérés comme de simples interfaces passives de saisie de texte couplées à des chaînes de compilation. Ils se métamorphosent en espaces de collaboration asynchrone entre l'intellect humain et des agents d'intelligence artificielle hautement autonomes. La conception d'un IDE natif IA, capable de rivaliser avec la réactivité algorithmique et la philosophie minimaliste d'éditeurs de texte classiques tels que Sublime Text, tout en intégrant la profondeur sémantique, la compréhension globale et la gestion contextuelle de systèmes modernes comme Cursor, exige une architecture hybride d'une grande complexité. Cette architecture doit impérativement réconcilier deux contraintes physiques et computationnelles qui semblent, de prime abord, diamétralement opposées. D'une part, le système doit garantir une latence de rendu ultra-faible pour l'interaction humaine, nécessitant un cycle de rafraîchissement inférieur à 8,33 millisecondes par itération pour maintenir une fluidité parfaite à 120 images par seconde.1 D'autre part, il doit orchestrer de manière transparente et non bloquante la gestion massive de données non structurées, le traitement de requêtes réseau asynchrones et l'indexation sémantique continue pour alimenter la mémoire de travail des modèles de langage (LLM) de la manière la plus efficiente possible.2

L'analyse technique approfondie qui constitue ce rapport déconstruit systématiquement les couches architecturales et logicielles nécessaires pour bâtir un tel système fondamentalement "from scratch". Ce document explore les décisions d'ingénierie critiques concernant les structures de données sous-jacentes, les moteurs de rendu matériel de nouvelle génération, les protocoles standardisés d'interopérabilité contextuelle, les pipelines d'indexation sémantique et les modèles d'orchestration multithread requis pour soutenir des flux de travail agentiques complexes.

## **Structures de Données Lexicales : Les Fondations Mathématiques de l'Édition Haute Performance**

La pierre angulaire de tout éditeur de texte, et a fortiori d'un IDE natif IA, réside dans la structure de données utilisée pour stocker, manipuler et interroger le tampon de texte principal en mémoire vive. Le choix de cette structure dicte non seulement les limites théoriques de la performance globale de l'IDE face à des fichiers sources volumineux, mais il détermine également sa capacité à supporter des éditions multi-curseurs fluides et des modifications asynchrones massives générées par des agents IA opérant en arrière-plan.

L'histoire de l'ingénierie des éditeurs de texte révèle que le stockage du texte sous la forme d'un simple tableau contigu (array) de caractères ou d'octets s'avère profondément inefficace pour les opérations d'édition réelles et intensives.5 Bien que des éditeurs textuels minimalistes et populaires, tels que Micro, utilisent des tableaux de lignes avec un certain succès pour des tâches simples du quotidien 5, cette approche naïve montre ses limites algorithmiques dès que l'échelle augmente. L'insertion ou la suppression d'un seul caractère au tout début d'un fichier monolithique de 100 mégaoctets stocké dans un tableau contigu nécessite le déplacement physique en mémoire de l'intégralité des millions d'octets subséquents. Cette opération génère une complexité temporelle asymptotique de ![][image1], ce qui provoque des blocages du fil d'exécution principal inacceptables pour un système visant une latence perceptuelle nulle.5

L'approche structurelle du "Gap Buffer" (tampon à espacement), historiquement popularisée par l'éditeur Emacs, résout élégamment ce problème d'insertion locale en introduisant un espace vide contigu (le "gap") directement à la position actuelle du curseur de l'utilisateur. Grâce à cette ingéniosité, les insertions successives à un endroit précis deviennent extrêmement rapides et bénéficient d'une excellente localité de cache spatial. Cependant, le déplacement du curseur sur de longues distances, par exemple d'une extrémité à l'autre d'un très gros fichier, dégrade drastiquement les performances globales en raison de la nécessité impérieuse de copier physiquement ce "gap" à travers l'espace mémoire pour l'aligner avec la nouvelle position du curseur.7 Pour un agent IA qui analyse et modifie simultanément de multiples sections d'un code source à des emplacements disparates, le Gap Buffer constitue un goulot d'étranglement majeur.

Pour pallier ces limitations historiques, la "Piece Table" (Table de Pièces) s'est imposée comme une norme industrielle transitoire, notablement plébiscitée par l'architecture initiale de Microsoft Word et adoptée par les premières versions de Visual Studio Code.6 Cette structure maintient deux tampons de données distincts : un tampon en lecture seule contenant le contenu initial du fichier chargé depuis le disque, et un tampon d'ajout ("append-only") réservé exclusivement aux nouvelles saisies de l'utilisateur. Le document textuel final est alors virtuellement représenté par une séquence ordonnée de "pièces" (des descripteurs ou pointeurs) pointant vers des segments de ces deux tampons statiques.6 L'avantage majeur et immédiat de la Piece Table réside dans la rapidité fulgurante de ses opérations d'annulation et de rétablissement (undo/redo), l'efficacité de sa gestion de l'encodage UTF-8, et son support natif des opérations multi-curseurs, puisqu'aucune donnée n'est jamais écrasée.6 Néanmoins, une session d'édition prolongée au sein d'un même fichier fragmente inexorablement la table de pièces originelle en des dizaines de milliers de petites pièces individuelles, ce qui ralentit de manière linéaire les temps de parcours et de rendu visuel. Pour surmonter cette fragmentation délétère, une structure hybride et sophistiquée appelée "Piece Tree" (Arbre de Pièces) a été conceptualisée et déployée.6 En organisant les pièces descriptives sous la forme d'un arbre binaire de recherche auto-équilibré (généralement un arbre Rouge-Noir), la complexité des recherches spatiales, des insertions et des modifications est ramenée d'un temps linéaire ![][image1] à un temps logarithmique ![][image2], combinant ainsi de manière synergique la flexibilité de la Piece Table avec une scalabilité computationnelle exceptionnelle.6

Cependant, pour un éditeur moderne de pointe axé sur des performances extrêmes et une interopérabilité IA sans faille (à l'instar d'architectures comme Helix ou Zed), la structure de données la plus avancée et privilégiée est la "Corde" (Rope).7 Une corde est une structure de données en forme d'arbre binaire dont les nœuds terminaux (les feuilles) contiennent de très courtes chaînes de caractères immuables (strings), et dont les nœuds internes stockent systématiquement la longueur totale combinée de tous leurs sous-arbres respectifs. Bien que la littérature académique et pratique souligne que les cordes peuvent être algorithmiquement complexes à implémenter, au point que même des implémentations de haute qualité en C ou C++ (comme celle de la SGI STL) puissent abriter des bugs subtils conduisant à des corruptions silencieuses d'état ou de contenu 5, l'adoption de langages modernes garantissant la sécurité de la mémoire à la compilation, comme Rust, atténue ces risques de manière drastique.5 Par exemple, des environnements récents exploitent les "SumTrees", une généralisation mathématique puissante de la structure de corde traditionnelle. Les SumTrees permettent d'indexer et d'agréger non seulement le texte brut, mais également des métadonnées sémantiques arbitraires de manière concomitante à chaque niveau de l'arbre.7 Cette approche s'avère technologiquement supérieure car l'immutabilité structurelle intrinsèque d'une Corde se marie parfaitement avec les architectures de rendu multithread modernes et les algorithmes de synchronisation sans verrou (lock-free).

| Structure de Données Lexicale | Complexité d'Insertion Locale | Complexité Pire Cas (Modification Distante) | Caractéristiques d'Empreinte Mémoire | Adaptabilité Multithread et IA Asynchrone | Exemples d'Utilisateurs Notables |
| :---- | :---- | :---- | :---- | :---- | :---- |
| **Tableau Contigu (Array)** | **![][image1]** (Copie des éléments) | ![][image1] (Déplacement total) | Très basse (Aucun overhead) | Inexistante (Nécessite des verrous globaux) | Micro |
| **Gap Buffer** | **![][image3]** amorti (Dans le gap) | ![][image1] (Déplacement du gap) | Basse (Espace du gap alloué) | Complexe (Données mutables partagées) | Emacs |
| **Table de Pièces / Arbre de Pièces** | **![][image3]** (Ajout en fin de buffer) | ![][image2] (Parcours de l'arbre équilibré) | Moyenne (Historique complet conservé en mémoire) | Modérée (Gestion délicate des index partagés) | Visual Studio Code, Microsoft Word |
| **Corde (Rope) / SumTree** | **![][image2]** (Reconstruction partielle de l'arbre) | ![][image2] (Reconstruction partielle de l'arbre) | Haute (Overhead des nœuds de l'arbre binaire) | Excellente (Immutabilité native favorisant le lock-free) | Zed, Helix, Bibliothèques Rust avancées |

L'analyse approfondie de ces structures de données fondamentales révèle un aperçu de second ordre absolument critique pour la conception d'un IDE : pour intégrer une intelligence artificielle capable de générer, d'analyser et de manipuler le code source de manière autonome, la structure de données sous-jacente doit impérativement supporter des modifications concurrentes et non destructives. Un agent IA génératif agissant en toile de fond modifiera fréquemment le code à des emplacements totalement éloignés de la position actuelle du curseur physique de l'utilisateur humain. Seules les architectures basées sur des Arbres de Pièces hautement optimisés ou des Cordes immuables, grâce à leur capacité intrinsèque à gérer l'édition non locale en temps purement logarithmique sans jamais perturber ni bloquer le fil de rendu visuel principal, peuvent soutenir de manière viable le paradigme d'une IA agentique agissant comme un véritable co-programmeur asynchrone.6 L'adoption d'une implémentation de type Rope ou SumTree en Rust s'impose donc comme le choix architectural fondamental pour la gestion du texte en mémoire de l'IDE cible.

## **Moteurs de Rendu et Architectures Visuelles à Latence Zéro**

La réactivité perçue d'un éditeur de texte définit de manière disproportionnée l'expérience psychologique et le flux cognitif du développeur. La latence absolue mesurée entre la frappe physique d'une touche sur le clavier et l'affichage effectif du pixel modifié à l'écran (souvent appelée "glass-to-glass latency") doit idéalement demeurer imperceptible pour l'œil humain. Deux philosophies d'architecture d'interface utilisateur (UI) distinctes dominent actuellement le spectre de la haute performance dans ce domaine : le rendu logiciel hautement optimisé (le modèle éprouvé de Sublime Text) et le rendu massivement parallèle accéléré par GPU (le modèle émergent de Zed). Comprendre leurs divergences est essentiel pour bâtir une interface capable d'encaisser la densité d'information d'un IDE IA.

### **L'Abastraction Polymorphique et le Rendu Logiciel Optimisé**

Sublime Text a historiquement bâti sa réputation incontestée de rapidité sur une architecture interne développée à 99 % en langage C++, s'appuyant sur un framework d'interface utilisateur fait maison et strictement propriétaire baptisé "Sublime GUI".10 Le créateur de cet éditeur a opté pour la création d'une trousse à outils (toolkit) personnalisée en anticipant que la vaste majorité des éléments graphiques critiques de l'interface, tels que le contrôle de rendu du texte massif et la gestion fluide des multiples onglets, nécessiteraient de toute façon des implémentations sur mesure, quel que soit le framework standard initialement choisi.10

Contrairement à la croyance populaire et persistante qui voudrait que de telles performances de fluidité nécessitent obligatoirement une accélération matérielle complexe via OpenGL ou DirectX, des analyses approfondies de débogage (réalisées à l'aide d'outils de profilage tels que gDEbugger ou Fraps) révèlent que Sublime Text n'effectue strictement aucun appel OpenGL pour le rendu de son interface sous Windows.10 L'architecture s'appuie en réalité sur les mécanismes de rendu logiciel natifs fournis par chaque système d'exploitation, tels que GDI+ sur les environnements Windows et XLib ou GTK pour la déclinaison Linux.10

La véritable clé de voûte de cette architecture performante réside dans la conception de sa couche d'abstraction multiplateforme. Au lieu de concevoir cette couche comme le "plus petit dénominateur commun" des différents systèmes d'exploitation (ce qui limiterait drastiquement les fonctionnalités), elle est implémentée comme une véritable union des fonctionnalités natives supérieures.10 Le code spécifiquement dépendant de l'OS (comme la gestion de bas niveau de la création de fenêtres système, l'interception des événements de souris, ou la boucle d'événements principale) est rigoureusement isolé dans une fraction minime de la bibliothèque de base, maintenant ainsi plus de 98 % de la base de code applicative totalement indépendante de la plateforme.10 Cette stratégie architecturale d'isolation permet d'éviter l'utilisation excessive et chaotique de directives de préprocesseur (les blocs \#ifdef windows ou \#ifdef macosx) qui finissent inexorablement par rendre un code source monolithique totalement inmaintenable. Les widgets personnalisés et la logique métier sont ensuite construits de manière polymorphique au-dessus de cette abstraction propre.10 Bien que cette architecture logicielle soit prodigieusement efficace pour minimiser l'empreinte mémoire et maximiser l'efficacité de la batterie sur des machines portables, elle atteint des limites physiques incontournables lorsqu'il s'agit de gérer des animations complexes, des superpositions de calques translucides (layers), ou des rendus à des fréquences d'affichage très élevées typiques des moniteurs modernes.

### **Le Paradigme Vidéoludique : Rendu GPU Direct et Shaders Personnalisés**

À l'opposé diamétral de l'approche purement logicielle en C++, la toute nouvelle génération d'éditeurs de code axés sur la performance absolue (dont Zed est le fer de lance) adopte une architecture visuelle structurellement calquée sur celle des moteurs de jeux vidéo tridimensionnels.1 L'objectif affiché est de maintenir un taux de rafraîchissement constant de 120 images par seconde (FPS) sur des écrans à haute fréquence, signifiant que l'application ne dispose mathématiquement que de 8,33 millisecondes par trame pour accomplir un cycle complet : mettre à jour l'état interne de l'application, calculer la disposition spatiale (layout) de tous les éléments d'interface, et écrire les données pixeliques finales dans le tampon d'affichage (frame buffer).1 Les frameworks traditionnels basés sur la manipulation du Document Object Model (DOM), comme Electron, échouent de manière structurelle et inévitable à garantir cette constance temporelle. Cet échec s'explique par les blocages erratiques induits par le ramasse-miettes (garbage collection) de JavaScript et par les calculs de re-disposition dispendieux exécutés sur le processeur central (CPU).1

La solution technique radicale pour surmonter cette barrière des 8,33 millisecondes consiste à transférer l'intégralité de la charge de calcul graphique vers le processeur graphique (GPU) en utilisant des API de bas niveau modernes, rapides et multiplateformes (telles que WGPU au sein de l'écosystème Rust, ou l'API Metal native sur macOS), par l'intermédiaire de bibliothèques d'interface développées sur mesure, comme le framework GPUI.1 Au lieu de dépendre de bibliothèques de dessin 2D traditionnelles et sédentaires, l'ensemble de l'interface utilisateur est philosophiquement déconstruit en primitives géométriques brutes qui sont directement injectées dans des programmes graphiques spécialisés exécutés sur le GPU, appelés *Vertex Shaders* et *Fragment Shaders*.1

L'implémentation de ce paradigme s'appuie sur des mathématiques graphiques avancées pour générer chaque élément visuel avec un minimum de polygones 1 :

* **Rendu des Formes via les Fonctions de Distance Signée (SDF) :** Les rectangles, qui composent conceptuellement plus de 90 % des éléments d'une interface graphique (comprenant les boutons, les panneaux latéraux, les info-bulles), ne sont pas dessinés par la création de maillages complexes, mais sont mathématiquement définis par des Fonctions de Distance Signée (Signed Distance Functions) au sein des shaders.1 Pour un pixel donné, une fonction SDF retourne la distance euclidienne exacte le séparant du bord le plus proche de l'objet ; cette distance est exactement de zéro sur la bordure géométrique, et devient négative si le pixel se trouve à l'intérieur de la forme. L'approche est élégamment simplifiée en centrant virtuellement le rectangle à l'origine mathématique du repère et en utilisant des valeurs absolues, ce qui permet d'exploiter la symétrie naturelle à travers les quatre quadrants géométriques. Pour dessiner des rectangles aux coins harmonieusement arrondis (arrondis complexes à générer via des polygones standard sans provoquer d'effets d'escalier), le framework GPUI contracte virtuellement le rectangle mathématique en lui soustrayant le rayon de courbure désiré, calcule la distance du pixel jusqu'à ce nouveau point interne, puis soustrait de nouveau le rayon du résultat final. Dans le pipeline graphique, un *vertex shader* se contente de définir une boîte englobante minimale (bounding box) en utilisant seulement deux triangles élémentaires. Le *fragment shader* prend ensuite le relais et s'exécute en parallèle pour absolument chaque pixel inclus dans cette boîte, calculant la fonction SDF spécifique pour déterminer avec une précision infinie si le pixel réside à l'intérieur du rectangle et lui appliquant la couleur d'arrière-plan appropriée.1  
* **Ombres Portées Analytiques par Convolution :** Rendre des ombres portées douces sur un GPU en utilisant des échantillonnages croisés de flou gaussien par pixel (Gaussian blur) est une opération extrêmement gourmande en ressources de calcul, susceptible de briser la contrainte des 8 millisecondes. Pour contourner ce problème, GPUI utilise une technique mathématique spécialisée et fermée (développée initialement par des chercheurs comme Evan Wallace).1 Pour les rectangles droits standards, le moteur utilise une solution analytique exacte impliquant la fonction d'erreur mathématique, notée ![][image4].1 Cette approche assimile la projection de l'ombre à une convolution mathématique d'une distribution gaussienne avec deux fonctions portes (Boxcar functions), une pour chaque axe dimensionnel de l'ombre. Dans le cas plus complexe des rectangles aux coins arrondis, qui ne sont pas mathématiquement séparables de manière stricte, l'algorithme procède par approximation de haute qualité : il exécute une convolution exacte le long d'un seul axe principal, puis fait glisser manuellement la distribution gaussienne le long de l'autre axe à plusieurs reprises pour simuler un flou parfaitement lisse et continu, le tout en un seul passage ultra-rapide dans le shader.1  
* **Rendu Typographique et Gestion de l'Atlas de Glyphes (Texture Atlas) :** Le rendu de polices de caractères lissées est l'opération la plus complexe d'un éditeur. Ce rendu est stratégiquement divisé en deux phases distinctes pour maximiser l'efficience.1 La phase initiale de "mise en forme" (text shaping), qui consiste à déterminer précisément où chaque caractère physique doit être positionné en tenant compte des ligatures complexes et de l'espacement kerning, est entièrement déléguée aux API natives et robustes du système d'exploitation sous-jacent (comme CoreText sur macOS).1 Le framework maintient un cache agressif associant paires de textes et polices ; si un segment de texte n'est pas modifié d'une trame à l'autre, cette étape computationnellement coûteuse de mise en forme est purement et simplement ignorée. Ensuite, au lieu de tenter de rendre laborieusement la typographie de manière vectorielle directement sur le GPU (une technique notoirement instable produisant des artefacts visuels sur les polices de très petite taille), le système utilise l'OS pour rastériser les glyphes vectoriels en pixels classiques. Cependant, seule la composante alpha (le canal d'opacité ou de transparence) de ces pixels rastérisés est conservée en mémoire. Ces composants alpha sont ensuite empilés et compressés à la volée dans une grande texture persistante vivant directement dans la mémoire vidéo (VRAM) du GPU, formant ce que l'on appelle un "Texture Atlas" via un algorithme de bin-packing.1 Pour contrecarrer les effets d'aliasing crénelé, l'éditeur calcule et stocke jusqu'à 16 variantes sous-pixels subtilement différentes pour chaque glyphe individuel.1 Au moment du rendu final, le *fragment shader* a l'avantage de pouvoir appliquer n'importe quelle couleur dynamique à un glyphe (pour la coloration syntaxique, par exemple) en multipliant simplement l'opacité alpha extraite de l'atlas par le vecteur couleur désiré, évitant drastiquement la nécessité absurde de stocker de multiples versions colorées du même caractère dans la mémoire vidéo.1

| Stratégie de Rendu d'Interface | Mécanisme Sous-jacent | Gestion Typographique | Avantages Architecturaux | Limitations Inhérentes | Utilisateurs Typiques |
| :---- | :---- | :---- | :---- | :---- | :---- |
| **Rendu Logiciel Natif** | Appels GDI+ / XLib / GTK orchestrés par C++ 10 | Intégrale via API système | Empreinte mémoire minime, portabilité aisée via couche d'abstraction.10 | Lutte pour maintenir un framerate constant lors d'animations complexes. | Sublime Text |
| **Accélération GPU (GPUI)** | Vertex/Fragment Shaders (SDFs, Erf) via WGPU/Metal 1 | Rastérisation OS \+ Atlas de Texture Alpha GPU 1 | Décharge massive du CPU, 120 FPS constant, animations fluides sans blocage.1 | Dépendance aux drivers GPU de l'utilisateur, complexité d'implémentation. | Zed |
| **Technologies Web (DOM)** | Moteur de rendu de navigateur (Chromium/Blink) encapsulé | Rendu CSS classique text-rendering | Développement ultra-rapide, immense écosystème de plugins existants. | Overhead mémoire colossal, pauses de garbage collection aléatoires.1 | Visual Studio Code, Atom |

L'analyse systémique de ces approches architecturales révèle une implication majeure pour un IDE embarquant une IA : pour intégrer de l'intelligence contextuelle continue sans jamais compromettre la fluidité du flux de travail humain (évitant ainsi le phénomène néfaste du "Don't Make Me Think Faster", où l'utilisateur est désorienté par une interface lente ou saccadée causée par des appels modèles lourds 13), l'architecture de rendu direct par GPU développée en Rust (type GPUI) est incontestablement supérieure. En externalisant la totalité de la charge de rendu sur la carte graphique, le processeur central (CPU) se retrouve libéré de ses chaînes, lui permettant de consacrer toute sa puissance disponible à l'analyse sémantique asynchrone, à l'indexation de fond et à la communication réseau ininterrompue avec les modèles de langage distants. L'interface applicative, conceptualisée via un trait d'abstraction géométrique (le trait Element comprenant les cycles de layout et paint descendant et ascendant 1), permet de superposer des indices visuels d'IA (comme des info-bulles générées dynamiquement ou des propositions de complétion multi-lignes translucides) simplement en poussant de nouveaux calques (Layer) dans la scène (Scene) virtuelle graphique, sans jamais impacter la performance de défilement du code.1

## **Analyse Syntaxique et Modélisation Sémantique via Tree-sitter**

Une interface graphique ultra-réactive, aussi sophistiquée soit-elle sur le plan matériel, ne constitue qu'un contenant stérile si l'environnement de développement ne possède pas une compréhension profonde et structurelle de la sémantique du code source qu'il affiche. Historiquement, et contrairement à un IDE lourd, un éditeur de texte classique utilise des cascades de moteurs d'expressions régulières (Regex) pour effectuer des tâches basiques comme la coloration syntaxique, l'auto-indentation ou le pliage de code (code folding). Cependant, un IDE natif IA, ambitionnant d'orchestrer des workflows complexes via des modèles génératifs, doit impérativement fournir à ses agents d'intelligence artificielle une représentation structurée, absolue et sémantiquement pure de l'ensemble de la base de code.14 C'est dans ce vide technologique critique qu'intervient la bibliothèque Tree-sitter.

### **Parsing Incrémental à Faible Latence et Tolérance aux Erreurs**

Tree-sitter n'est pas un simple utilitaire ; c'est un générateur universel d'analyseur syntaxique (parser-generator) couplé à une puissante bibliothèque de parsing incrémental.15 Écrit intentionnellement en langage C pur, sans aucune dépendance externe lourde, il est expressément conçu pour être universel (capable d'analyser n'importe quel langage de programmation moderne) et suffisamment rapide pour réanalyser l'intégralité de la structure d'un fichier source à chaque milliseconde, ou plus précisément à chaque frappe individuelle de l'utilisateur sur le clavier.15

L'avantage compétitif fondamental de Tree-sitter, et ce qui le rend indispensable pour un environnement IA, réside dans son incroyable tolérance aux erreurs de syntaxe.15 Dans un flux de développement réel, le code est par définition dans un état incomplet ou syntaxiquement malformé 90 % du temps pendant que l'utilisateur est en train de taper. Contrairement aux parseurs de compilateurs stricts qui s'arrêtent net et crashent à la première virgule manquante, Tree-sitter continue son analyse globale et génère des nœuds de type ERROR dans l'arbre syntaxique abstrait, encapsulant l'erreur tout en préservant l'analyse parfaite du reste du fichier.15 Pour un agent IA (notamment dans des outils modernes comme Aider), cette robustesse inébranlable permet d'ingérer le code instantanément. L'IDE peut simplement filtrer et utiliser spécifiquement ces nœuds ERROR isolés pour identifier de manière déterministe les lignes souffrant d'erreurs fatales, et les présenter directement à l'agent LLM dans un format hautement ciblé et digeste, augmentant exponentiellement sa capacité à comprendre le problème de compilation et à formuler la correction exacte.16

Cette mécanique de parsing permet de traduire instantanément et continuellement le flux de code brut en un Arbre Syntaxique Concret (CST), puis en Arbre Syntaxique Abstrait (AST).14 Chaque nœud logique composant cet arbre—qu'il s'agisse des définitions de fonctions globales, des déclarations de classes, des arguments de méthodes ou des limites précises des portées de variables (scopes)—est méticuleusement cartographié. Le parseur associe à chaque composant ses coordonnées spatiales exactes et immuables (positions en octets de départ et de fin, ligne, colonne) au sein du tampon de texte en mémoire.14

### **Extraction du Graphe de Dépendances et Élaboration du Contexte Ciblé**

En plus de la simple reconnaissance structurelle intra-fichier, les requêtes sémantiques formulées via Tree-sitter permettent à l'IDE de construire à la volée un véritable graphe de dépendances croisé.17 Lorsqu'un utilisateur demande à son partenaire IA d'optimiser, de documenter ou de déboguer une méthode spécifique au sein d'une classe complexe, le moteur d'analyse sous-jacent basé sur Tree-sitter ne se contente pas d'isoler la méthode visée en coupant grossièrement du texte. Il navigue astucieusement dans l'AST pour extraire de manière récursive toutes les fonctions, classes ou variables qui sont appelées et instanciées au sein même de la méthode cible.17

Une fois cette cartographie établie, l'algorithme interne classe l'importance de ces différentes sections de code collatérales en se basant sur des heuristiques telles que la fréquence de référence.17 L'outil peut ensuite interroger ces signatures et les aggréger. Par exemple, une requête Tree-sitter appliquée à du code source en langage Go (utilisant une grammaire (type\_declaration (type\_spec name: (type\_identifier) @name)) @definition.type) permettra d'isoler le bloc exact de déclaration d'une structure de données.19 La collecte chirurgicale de ces extraits (snippets) associés réduit drastiquement la pollution contextuelle, maximisant l'efficacité informationnelle du modèle de langage en injectant uniquement les fragments de code vitaux dans la fenêtre de prompt.19

Un aperçu de troisième ordre crucial se dégage de cette dynamique architecturale : l'AST généré dynamiquement devient la couche de traduction universelle et bidirectionnelle entre le paradigme d'ingénierie humain (qui lit et comprend le code visuellement et séquentiellement de haut en bas) et le paradigme d'intelligence artificielle (qui requiert structurellement un graphe relationnel dense et sémantiquement pondéré pour "raisonner" sans halluciner). Dans cette configuration, l'IDE n'agit plus en tant que simple éditeur de fichiers plats manipulables, mais évolue en une véritable interface d'interrogation de bases de données structurelles s'actualisant en temps réel.14 Cette architecture remplace de fait les cycles itératifs et inefficaces de "Globbing/Greping/Reading" (lire tous les dossiers, faire une recherche textuelle, lire le fichier) par un pipeline robuste d'investigation composé d'opérations natives de registre : extraction de structure globale, recherche de symboles parfaits, récupération des appels parents (callers) et consultation des implémentations exactes.20

## **Indexation Sémantique et Pipeline RAG (Retrieval-Augmented Generation)**

L'un des défis d'ingénierie les plus colossaux dans la conception fonctionnelle d'un "partenaire codeur IA" complet réside dans la capacité mécanique du modèle à comprendre et à prendre en compte l'ensemble d'un projet logiciel (qui peut contenir des centaines de fichiers et des centaines de milliers de lignes de code), une échelle qui dépasse largement et allègrement la taille maximale autorisée de sa fenêtre de contexte immédiate (généralement limitée entre 32k et 128k tokens pour les modèles de production courants). La solution standardisée dans l'industrie de l'IA applicative pour ce problème est l'architecture RAG (Retrieval-Augmented Generation).15 Cependant, l'application du RAG classique à des bases de code nécessite une réinvention technique complète.

### **Les Failles du Découpage Linéaire et l'Avènement du Chunking Sémantique**

La plupart des pipelines RAG traditionnels commencent par ingérer des documents et utilisent des séparateurs et heuristiques linéaires pour effectuer le "chunking" (le découpage du document en fragments gérables).23 Par exemple, ils vont aveuglément tronquer le texte textuellement tous les 1000 tokens d'affilée, ou scinder artificiellement les blocs à chaque occurrence d'un double saut de ligne. Pour du texte littéraire, cette approximation grossière est passable. Pour du code source logiciel hautement structuré, cette méthode s'avère profondément catastrophique.25 Le découpage heuristique linéaire fracture violemment les structures sémantiques fondamentales du programme. Il risque quotidiennement de séparer la déclaration d'une classe maîtresse de ses propres méthodes internes vitales, ou de tronquer des blocs conditionnels imbriqués essentiels, ce qui dégrade considérablement la qualité technique du code généré ultérieurement par l'IA lors de la phase de réponse.25

L'architecture optimale d'un IDE natif IA, exploitant la puissance décrite précédemment, exige l'intégration de Tree-sitter directement dans le pipeline d'ingestion RAG pour effectuer un "Chunking Sémantique" profond.21 Au lieu de fragmenter textuellement le fichier de manière arbitraire, le service d'indexation parcourt silencieusement en arrière-plan l'AST nouvellement généré et segmente mathématiquement le code selon ses propres frontières logiques intrinsèques.14 L'algorithme d'indexation va, par exemple, encapsuler intelligemment une fonction asynchrone entière et exhaustive dans un seul et unique "chunk" de données, tout en veillant méticuleusement à y lier les métadonnées déclaratives de sa classe parente et ses imports.14 Cette méthodologie de haute précision garantit absolument que, lors de la phase imminente de vectorisation (Embedding), la représentation numérique finale capturée dans le vecteur préserve de manière indélébile la véritable signification algorithmique, l'intention de conception et la portée contextuelle de l'extrait de code.24

### **Vectorisation, Base de Données Vectorielle et Recherche Sémantique**

Une fois le fractionnement sémantique réalisé et validé via l'arbre de syntaxe concret (CST) garantissant la préservation des tokens et limites originelles 14, chaque chunk extrait est méthodiquement transmis à un modèle d'embedding, tel qu'un modèle local open-source performant comme sentence-transformers/all-mpnet-base-v2 provenant de l'écosystème HuggingFace, ou via une API d'embedding spécialisée.22 Ce modèle d'apprentissage profond convertit l'extrait de code textuel complexe en un vecteur mathématique dense à plusieurs centaines de dimensions, encapsulant son sens sémantique profond plutôt que son lexique littéral.23

Ces vecteurs, accompagnés de leurs précieuses métadonnées structurelles (telles que le chemin du fichier, le numéro de ligne exact, et le type de nœud AST), sont subséquemment insérés et indexés de manière permanente au sein d'une base de données vectorielle locale et embarquée, telle que LanceDB, ou déléguée à des services managés comme Pinecone via des espaces de noms (namespaces) dédiés par projet.22 Lorsqu'un développeur exprime une requête fonctionnelle vague en langage naturel, par exemple "Où la logique de mise à jour du solde du portefeuille virtuel s'effectue-t-elle au sein de ce projet monolithique?", l'IDE convertit cette question en un vecteur interrogateur. Il procède alors à un calcul ultra-rapide de similarité cosinus au sein de la base de données vectorielle pour extraire les chunks sémantiques de code les plus pertinents.22 Ce processus sophistiqué remplace le cycle de recherche erratique basé sur de simples mots-clés par des recherches mathématiquement garanties, adossées à un index sémantique s'actualisant en continu grâce à la réactivité incrémentale du parseur AST sous-jacent.20

## **Ingénierie de la Fenêtre de Contexte et Découverte Dynamique (Le Modèle Cursor)**

Posséder un index vectoriel parfait ne garantit pas la production d'un code fonctionnel si l'injection des informations extraites submerge les capacités cognitives du modèle d'intelligence artificielle.19 La supériorité algorithmique d'éditeurs avant-gardistes tels que Cursor ne réside paradoxalement pas dans les modèles fondationnels d'IA qu'ils utilisent (qui sont pour la plupart strictement les mêmes modèles frontières, tels que Claude 3.5 Sonnet ou GPT-4o, disponibles universellement via les API publiques), mais bien dans leur maîtrise exceptionnelle de l'ingénierie de la fenêtre de contexte ("Context Window Management").3

Orchestrer l'intégration transparente d'une IA dans le flux d'un IDE s'apparente conceptuellement à l'exécution d'une recette culinaire de très haute précision : le système logiciel central doit jongler habilement et en temps réel entre les instructions système strictes (system prompts dictant la personnalité et les contraintes), les directives imprécises de l'utilisateur final, l'état fluctuant de l'arbre global des fichiers du projet, les erreurs de compilation instantanées, et les limites absolues de la mémoire de travail (le contexte) du modèle distant.3

### **La Saturation du Contexte et les Limites de l'Injection Statique**

Dans une architecture de développement assistée par IA de première génération, les concepteurs injectaient aveuglément et massivement de multiples fichiers entiers ou d'immenses sorties de journalisation de terminal directement dans le prompt initial avant de le soumettre au modèle (une approche rudimentaire qualifiée de "statique").4 Bien que fonctionnelle sur de minuscules scripts, cette méthode sature inévitablement et très rapidement la capacité maximale de la fenêtre de contexte. À mesure que les itérations de conversation en aller-retour (les "turns") avec l'agent programmateur se prolongent sur plusieurs dizaines de minutes ou d'heures, le contexte historique s'allonge de manière dramatique et continue. Cette accumulation provoque une dégradation notable de la qualité de la réponse et une augmentation de l'incohérence, un phénomène algorithmique empirique tout à fait comparable à la perte de mémoire à court terme dans de très longues interactions sociales humaines.3

De plus, chaque token supplémentaire expédié allonge linéairement la latence de première réponse (Time To First Token) et accroît exponentiellement les coûts d'inférence. Historiquement, lorsque la limite stricte de contexte est atteinte, le système se voit contraint de déclencher des algorithmes de compression d'historique avec perte (des résumés de conversation générés par LLM). Ce processus de compression "lossy" est fondamentalement délétère en ingénierie logicielle car l'agent en vient inévitablement à oublier ou à lisser des détails techniques ultra-critiques relatifs à la tâche de programmation fine qu'il devait accomplir, provoquant des hallucinations cycliques.4

### **La Découverte Dynamique : Fichiers Virtuels, Historique et Terminaux Intégrés**

L'architecture révolutionnaire déployée par les environnements s'inspirant de Cursor introduit et consacre un nouveau paradigme architectural nommé "Découverte Dynamique du Contexte" (Dynamic Context Discovery).4 L'objectif directeur de cette ingénierie est minimaliste : fournir initialement au modèle le strict minimum de détails nécessaires (un contexte statique extrêmement compressé) tout en le dotant simultanément d'un panel d'outils interactifs lui permettant de "tirer" (pull) lui-même l'information granulaire dont il a spécifiquement besoin, et uniquement au moment opportun où il en a besoin.4

* **L'Historique de Chat comme Système de Fichiers :** Au lieu de résumer de force l'historique de la conversation et de perdre irrémédiablement des états logiques passés, l'IDE synchronise et traite l'historique textuel intégral du chat comme un véritable fichier virtuel persistant sur le système de la machine locale.4 Lorsque la fenêtre de contexte atteint sa capacité limite et bascule, le système insère intelligemment dans la mémoire résiduelle de l'agent IA une simple référence (un pointeur de chemin) vers ce fichier de sauvegarde d'historique. Si, au cours d'un raisonnement complexe ultérieur, l'agent constate logiquement qu'il lui manque une information architecturale critique en raison de la compression récente de son contexte immédiat, il possède la liberté cognitive et l'outil pour exécuter dynamiquement une recherche ciblée (en utilisant un utilitaire natif comme grep ou en invoquant la recherche sémantique de l'IDE) au sein même de ce fichier historique passé pour recouvrer l'information perdue.4  
* **Synchronisation du Terminal et Fichiers de Trace (Logs) :** Suivant une logique identique et tout aussi critique, l'IDE moderne ne se contente plus d'afficher un émulateur de terminal ; il synchronise en permanence la sortie standard (stdout) et la sortie d'erreur (stderr) de ce terminal intégré de développement vers des fichiers de traces locaux enregistrés sur le disque.4 Au lieu d'imposer à l'utilisateur humain la tâche fastidieuse de copier-coller manuellement les milliers de lignes d'erreur d'une compilation défectueuse ou le journal craché par un processus de serveur Node.js tournant en continu, l'utilisateur demande simplement en langage naturel à son IA : "Pourquoi l'exécution de ma commande a-t-elle échoué?". L'agent IA, doté d'outils de manipulation de fichiers, utilise alors de manière totalement autonome un appel système de type tail pour examiner chirurgicalement les toutes dernières lignes de la sortie du terminal. Il parvient à identifier l'erreur fatale (souvent située dans les 50 dernières lignes) sans jamais avoir à ingérer dans sa fenêtre de contexte statique l'entièreté d'un fichier de log massif pouvant peser plusieurs dizaines de mégaoctets, reflétant ainsi un comportement de diagnostic analytique de qualité professionnelle.4

## **Standardisation de l'Interopérabilité : Le Protocole MCP (Model Context Protocol)**

Si l'indexation locale, la cartographie des arbres syntaxiques et la surveillance du terminal constituent l'omniscience immédiate et le cerveau cognitif de l'IDE, cet environnement de développement ne doit en aucun cas vivre en vase clos. Pour transcender le statut de simple éditeur intelligent et devenir un véritable assistant ingénieur, un "partenaire codeur" doit impérativement avoir la capacité de dialoguer et d'interagir activement avec une pléthore de systèmes d'entreprise externes : messageries d'équipes (Slack), clusters de bases de données relationnelles en production (PostgreSQL), dépôts de code sources en nuage (GitHub, GitLab), chaînes d'intégration continue (CI/CD), corpus de documentation distante, ou même des référentiels de conception d'interfaces graphiques (Figma).28

### **La Résolution du Problème d'Intégration Exponentiel (![][image5])**

Dans le paradigme logiciel de l'année 2024 et des années antérieures, connecter un modèle d'IA génératif à chacune de ces sources d'information disparates exigeait systématiquement de la part des développeurs l'écriture laborieuse d'une "colle" logicielle (glue code) fragile, personnalisée et hautement spécifique pour chaque combinaison possible. Ce fonctionnement fragmenté créait inévitablement un cauchemar architectural, un problème de mise à l'échelle exponentiel décrit par l'équation de complexité ![][image6], un écosystème extrêmement fragile, redondant et impossible à maintenir sécuritairement au niveau de l'entreprise.30

Pour rompre ce goulot d'étranglement évolutif, l'industrie a coalescé et s'oriente aujourd'hui vers l'adoption d'un standard universel et ouvert : le Model Context Protocol (MCP), initialement mis en open source par les laboratoires d'Anthropic.29 Le protocole MCP agit conceptuellement et fonctionnellement comme l'équivalent parfait d'un port "USB-C" universel pour l'écosystème de l'intelligence artificielle.30 Peu importe qui a manufacturé l'IDE (le "Client IA") ou quel fournisseur maintient le logiciel de base de données (le "Serveur source"). Tant que ces deux entités indépendantes adhèrent strictement au vocabulaire et à la grammaire de ce protocole standardisé, elles s'interconnectent et dialoguent instantanément sans aucun développement supplémentaire.30 MCP définit formellement une architecture bidirectionnelle robuste s'appuyant sur des messages structurés au format JSON-RPC 2.0 pour encadrer et garantir un échange fluide de requêtes et de données brutes entre l'IA interrogatrice et les outils exécutifs.31

Dans l'optique de la conception de l'architecture d'un IDE personnalisé et souverain, le cœur de ce système applicatif se comportera structurellement en tant que **Client MCP** principal.31 Cette abstraction paradigmatique implique que le noyau monolithique de l'IDE n'a plus aucune obligation d'apprendre ou d'embarquer les bibliothèques et SDK nécessaires pour comprendre comment ouvrir un socket vers une base de données MySQL distante ou comment formater précisément une requête GraphQL vers l'API complexe de GitHub. Le noyau de l'IDE se contente désormais de parler fluidement et uniformément le langage du protocole MCP aux divers **Serveurs MCP** (soit instanciés en processus locaux, soit hébergés sur des serveurs distants), ces derniers encapsulant et gérant en totale isolation l'interaction native et complexe avec leurs systèmes de données respectifs.30

### **Transports Sécurisés (stdio), Modération de Contexte et Implémentation Rust**

La véritable puissance architecturale de sécurité et de déploiement du MCP réside dans la flexibilité intrinsèque de sa couche de transport de données. Les connexions MCP peuvent s'établir traditionnellement via des requêtes HTTP ou des Server-Sent Events (SSE) (une approche idéale pour fédérer des sources de données publiques en cloud), mais, de manière exponentiellement plus critique pour l'intégrité et la confidentialité inhérentes au développement logiciel d'entreprise privé, la liaison peut s'opérer de manière strictement locale via des processus attachés aux flux d'entrées/sorties standards du système, connus sous le nom de flux stdio (standard input/output).28

Lorsqu'un serveur d'accès MCP spécifique (par exemple, un explorateur de système de fichiers local ou un interfaçage vers une base SQLite contenant des données d'utilisateurs confidentielles) utilise le transport stdio, il s'exécute de manière cloîtrée en tant que sous-processus totalement local sur la machine du développeur, communiquant avec l'IDE maître (le client) par simple passage de données dans la mémoire de l'OS, sans jamais exposer ces interactions vitales, ces données ou les requêtes SQL au réseau internet grand ouvert.31 L'IDE se contente d'instancier le serveur en tant que processus fils sécurisé.34

Toutefois, une problématique d'ingénierie inattendue et de taille émerge inéluctablement avec l'intégration agressive de MCP dans des flux de travail réels : la prolifération. L'accumulation effrénée de serveurs MCP connectés à l'IDE entraîne une surcharge immédiate de la fenêtre de contexte allouée au LLM.4 En effet, par défaut, le protocole transmet intégralement les descriptions textuelles extensives et les schémas JSON complexes de *chaque* outil exposé au modèle afin de l'informer de ses capacités. L'ingénierie avancée visant à contourner cet écueil, largement inspirée par les mécanismes optimisés de Cursor, exige de stocker physiquement et séparément les descriptions d'API des outils MCP au sein de dossiers locaux distincts sur le disque dur.4 L'agent d'intelligence artificielle n'est initialement instancié et pourvu que des "noms" bruts des outils disponibles (fournissant un contexte statique ultra-minimal). S'il détermine lors de son raisonnement heuristique qu'il a effectivement besoin de comprendre les détails opérationnels et le schéma de paramètres d'un outil particulier (par exemple, l'outil query\_db), il utilise silencieusement un appel asynchrone pour lire, par le biais d'outils filtres de système d'exploitation intégrés comme jq (pour le parsing JSON) ou ripgrep (pour la recherche rapide), les schémas d'interaction requis, économisant ainsi de manière démontrable près de 46,9 % de l'enveloppe précieuse des tokens de la fenêtre de contexte principale.4

| Composant Architectural | Typologie d'Implémentation Standard | Rôle Explicite dans l'Architecture de l'IDE IA |
| :---- | :---- | :---- |
| **Client MCP Intégré** | Cœur natif de l'IDE (C++ ou Rust performant) | Intercepte les requêtes planifiées de l'agent IA, route dynamiquement les commandes vers les serveurs enregistrés appropriés.30 |
| **Couche de Transport** | Processus locaux (stdio) ou Connexions distantes (HTTP/SSE) | Garantit la sécurité granulaire et l'isolation mémoire. Empêche drastiquement l'exfiltration accidentelle de code source propriétaire sur des réseaux ouverts.28 |
| **Serveur MCP (Local)** | Sous-processus exécutable géré par l'IDE (ex: via cargo run) | Exécute des commandes shell dangereuses, explore l'arbre de dépendance des fichiers, interagit lourdement avec le compilateur C++ ou le linker système.31 |
| **Serveur MCP (Distant)** | API Web ou Microservice interne d'entreprise | Récupère de la documentation technique systématiquement à jour (ex. docs framework React), intègre des commentaires depuis des tickets Jira/GitHub à la volée.28 |

Pour garantir l'harmonie, la sécurité d'exécution et la constance absolue de performance entre cet IDE naissant (idéalement forgé en Rust pour surpasser la mémoire sujette aux fuites du C++) et le traitement frénétique des interactions d'outils massives, la création structurelle de clients et de serveurs MCP peut et devrait s'effectuer nativement en langage Rust par l'utilisation de la crate spécialisée rmcp.33 L'écosystème mature de Rust, sublimé par l'utilisation intensive du moteur asynchrone tokio pour un I/O (Entrées/Sorties) non bloquant et de macros déclaratives surpuissantes de dérivation (telles que \#\[tool\_handler\]), permet à l'ingénieur de l'IDE d'exposer automatiquement des fonctions internes hautement complexes (comme une exploration récursive de la table de fichiers ou des requêtes réseau directes vers un testeur DNS) sous une forme instantanément compréhensible par une machine IA, générant dynamiquement à la compilation les schémas d'entrée exacts requis (via la bibliothèque schemars).33

La conséquence philosophique et architecturale fondamentale de cette intégration massive et native du protocole MCP est que l'IDE n'est plus envisagé comme un progiciel monolithique et lourd, doté intrinsèquement de toutes les capacités d'analyse possibles. Il devient au contraire un "orchestrateur générique" universel, léger, à latence nulle.29 Les fonctionnalités spécifiques de l'environnement, qui jadis alourdissaient l'application (comme le linting approfondi, la gestion des dépendances Git complexe, ou l'accès sécurisé aux bases de données locales), sont désormais des entités entièrement modulaires, instanciées et traitées isolément par des processus serveurs MCP tournant en marge, décentralisant par la même occasion le risque systémique de plantage critique de l'interface utilisateur graphique principale.29

## **Orchestration Agentique, Multithreading et Conception d'API Internes**

L'avènement du flux de travail "agentique" (agentic workflow) transforme de fond en comble notre perception technique et temporelle de l'exécution d'un logiciel d'assistance à la création de code. Une session de développement augmentée par l'intelligence artificielle ne se limite désormais plus à la formulation de petites requêtes sporadiques et rapides d'auto-complétion (autocomplete simple de ligne) dont la latence d'exécution totale se chiffre en quelques millisecondes ou secondes, comme le faisaient les premières itérations limitées de systèmes comme GitHub Copilot.36 De très récentes évaluations empiriques méticuleuses, menées par l'organisme METR en août 2025 sur des systèmes frontières avancés, ont indubitablement démontré la capacité foudroyante des modèles de langage à maintenir et à soutenir un fil de raisonnement logique ininterrompu et continu pendant 2 heures et 17 minutes, accomplissant des cycles itératifs autonomes vertigineux de planification de l'architecture logicielle, de rédaction massive de code à travers des dizaines de fichiers, d'exécution de suites de tests unitaires, de correction récursive d'erreurs révélées et de refactorisation en boucle de la structure du système jusqu'à validation mathématique de la solution.36

### **Parallélisation Extrême : RenderThread contre Moteur d'Orchestration Agentique**

Au niveau micro-architectural, si une tâche heuristique extraordinairement complexe de refactorisation d'un module entier, orchestrée par un agent IA fonctionnant de manière frénétique en arrière-plan, venait à être par erreur traitée sur le même fil d'exécution matériel (le même thread) que l'interface graphique de rendu, l'IDE se retrouverait inexorablement et fatalement figé, figeant la boucle d'interaction humaine ("Application Not Responding"). Il est donc d'une importance critique, dès l'élaboration des fondations de l'application, de bâtir une architecture asynchrone massivement multithread, cloisonnée et hautement séparée, en appliquant des modèles d'exécution en file d'attente (Task Queues).38

L'architecture interne robuste de l'IDE doit encapsuler sa logique métier et spatiale dans une matrice multithread cloisonnée :

1. **Le RenderThread (Fil de Rendu Principal exclusif) :** Ce thread est un sanctuaire algorithmique, strictement et impitoyablement réservé au rafraîchissement des primitives graphiques (via les shaders et l'accélération matérielle GPUI/WGPU visant un maintien de 120 FPS constant).39 Pour garantir cette constance à moins de 8,33 ms, absolument aucune opération complexe d'Entrée/Sortie (E/S ou I/O) sur le disque de stockage, aucune requête de réseau web vers l'API OpenAI ou Anthropic, ni aucun traitement synchrone de l'énorme arbre Tree-sitter ne doit y être programmé ou toléré.39  
2. **Le ThreadPool de Service Continu (Language Servers et Parsing) :** Des réserves de fils d'exécution (Thread Pools contenant des "Background Workers") gèrent en totale isolation asynchrone l'analyse incrémentale complexe de l'AST (via Tree-sitter) déclenchée de manière transparente à chaque frappe discrète au clavier du développeur.38 Ces travailleurs de l'ombre informent, via des canaux de communication non bloquants, le RenderThread principal de mettre à jour le style visuel de l'écran (par exemple, re-coloriser un nouveau bloc syntaxique valide) de manière différée et indolore.40  
3. **Le Agent Orchestration Engine (Moteur Dédié d'Orchestration Agentique) :** Ce composant logiciel architectural constitue le cœur asynchrone révolutionnaire du projet. Ce super-service, tournant continuellement en arrière-plan et souvent orchestré via un runtime asynchrone puissant (comme tokio en Rust), maintient en vie et coordonne la boucle d'état cognitive de l'agent d'IA.33 Ce moteur gère scrupuleusement les files d'attente des requêtes et réponses vers les LLMs massifs (via des appels réseau), supervise les temps d'attente (timeouts), déclenche l'exécution isolée des processus serveurs MCP distants, et parse leurs résultats bruts.38

Par ailleurs, un pan vital de ce moteur d'orchestration réside dans sa fonction de gestionnaire des modifications projetées ("Task Planning"). Lorsqu'un agent intelligent génère des modifications structurelles étendues qui altèrent brutalement de multiples fichiers interdépendants (par exemple, lors d'une mission formulée comme "Ajouter un mode multijoueur synchrone à ce gestionnaire d'application web existant" 42), l'IDE en tant qu'entité ne doit en aucun cas altérer directement le tampon de texte actuellement visualisé par l'utilisateur, sous peine de détruire le travail en cours. Il doit modéliser et appliquer conceptuellement l'ensemble de ces modifications IA au sein d'une structure de donnée arborescente fantôme de type "Worktree" parallèle. Cela garantit une imperméabilité entre le "brouillon de l'IA" et le "bureau de l'humain" (évitant de corrompre de manière asynchrone le code activement tapé par le développeur humain). Une fois la séquence agentique monumentale virtuellement achevée (après peut-être 30 minutes de travail en tâche de fond), l'IDE consolide les changements atomiques et les présente sous la forme ergonomique d'un panneau de "Mission Control" proposant une révision granulaire et comparative des différences (diffing) via un système de "diffs" par interface unifiée, permettant une validation sécurisée et compartimentée.42

### **Ergonomie Temporelle de l'IA : Le Paradigme du "Don't Make Me Think Faster"**

L'orchestration agentique multithread répond également à une contrainte majeure de l'ergonomie (User Experience, UX) face aux longs processus de génération. Le concepteur de l'interface doit embrasser le corollaire perceptuel "Don't Make Me Think Faster", concept dérivé de l'illustre adage d'ergonomie "Don't Make Me Think".13 Ce concept, crucial pour l'adoption des interfaces basées sur les LLM, stipule formellement qu'il est profondément néfaste de forcer l'utilisateur humain à aligner artificiellement son rythme cognitif biologique ou ses attentes psychologiques sur la vitesse d'exécution erratique de la machine. Qu'il s'agisse d'un délai d'attente de réponse IA instantané ou de "l'angoisse" ressentie face à une tâche agentique complexe s'échelonnant sur des heures, le développeur humain demeure un "Chronosapien" dont la cognition temporelle est limitée et structurée par habitudes.13

Si la création logicielle d'une simple fonction math.add() nécessite une réponse en une demi-seconde (interaction de type dialogue direct synchrone), la requête ambitieuse visant à "Générer la suite complète de tests de conformité, documenter les modifications et créer un ticket de revue PR fonctionnelle" requiert un temps d'exécution se rapprochant psychologiquement de la gestion de projets par traitement par lots (batch processing) telle que pratiquée dans les années 1960\.13 L'interface de l'IDE IA doit donc, au niveau conceptuel et visuel, muter d'un modèle de "tchat persistant hyperactif" vers une interface d'orchestration délégative où le suivi d'exécution (la supervision et la réflexion en toile de fond) s'effectue hors-bande, via de discrets indicateurs d'état asynchrones (progress bars invisibles, notifications sonores, tableaux de bord de révision finalisés), pour ne pas encombrer ni interrompre le focus central visuel du programmeur humain sur son propre fragment de code.13

### **Principes Rigoureux de Conception d'API Internes pour la Consommation Agentique**

Afin de permettre à l'agent IA (que ce soit par l'interface externe du Model Context Protocol ou par un SDK interne étroitement couplé) de naviguer intelligemment dans les dossiers de l'arborescence, de diagnostiquer les failles de logique du programme, et de manipuler le système de fichiers, les interfaces de programmation applicative internes de l'IDE (les fameuses API de l'outil) doivent être philosophiquement repensées et explicitement conçues pour des entités consommatrices non humaines.46 C'est une erreur d'ingénierie commune de postuler qu'une IA peut exploiter convenablement une API conçue pour être lue et traitée par une interface Web classique construite pour un œil humain. Fournir une API "traditionnelle" non structurée à un agent LLM mène statistiquement, inéluctablement, à des dysfonctionnements, des boucles infinies de répétition (loops) et de graves hallucinations cognitives de la machine.48

Les principes fondamentaux de l'architecture d'une telle API interne "Agent-Friendly" doivent obéir impitoyablement aux directives architecturales suivantes 46 :

* **Agnosticisme et Idempotence Absolue des Opérations Mutatives :** Toutes les routes des API internes d'édition directe du tampon de code source, de création de dossiers virtuels ou d'injections de requêtes système (écritures en base de données) doivent être strictement idempotentes.46 Elles ne doivent conserver ni état implicite invisible ni dépendances résiduelles en mémoire. Cette caractéristique est cruciale : si une connexion d'agent IA subit un délai d'attente réseau (timeout) ou une faille de raisonnement, et qu'il relance de manière aveugle exactement la même commande de modification d'insertion (pensant qu'elle a échoué la première fois), le système sous-jacent de l'IDE ne doit en aucun cas créer un doublon corrompu de l'insertion.46 L'exécution multiple d'une même commande doit garantir indéfectiblement le même état final unifié que l'exécution d'une commande unique.  
* **Retours d'Erreurs Sémantiques Formellement Structurés :** Dans l'hypothèse où l'exécution d'une commande par l'agent échoue fatalement (par exemple, si l'IA tente insidieusement de compiler un code nouvellement généré par ses soins mais comportant une variable non initialisée ou une erreur de type détectée), l'API de compilation interne ne doit jamais simplement crasher la boucle ou renvoyer un laconique code binaire de sortie (exit code 1). Elle doit intercepter avec diligence l'erreur produite (idéalement via la récupération du nœud spécifique ERROR au sein de l'arbre sémantique Tree-sitter susmentionné 16). Elle doit ensuite impérativement formater et renvoyer cette erreur vers le payload de l'LLM sous une structure JSON prévisible contenant : un message de guidage formaté explicitement pour l'ingestion par un modèle d'apprentissage, le type sémantique de l'erreur, et une suggestion heuristique orientée sur les limites dépassées (par exemple "Fichier cible hors du répertoire autorisé par les stratégies de sécurité").46 Cette architecture défensive structurée ("structured errors" 50) est la seule méthode empirique permettant de forcer la boucle de contrôle itérative du LLM à s'auto-corriger efficacement par raisonnement successif, accomplissant le très prisé paradigme de réflexion intelligente ("Self-Correction / Reflection Agent Pattern") sans nécessiter d'intervention humaine de sauvetage.51  
* **Protection Limitative, Restrictions d'Accès et Pagination Active :** Le programmeur d'un environnement IA doit être intrinsèquement conscient que chaque ligne de résultat, ou chaque octet textuel renvoyé par un outil interne vers le client IA, consomme directement de très coûteux "tokens" facturables via l'API (que ce soit pour le fournisseur ou le modèle déployé en local).49 Plus dangereusement encore, un surplus d'informations sature et écrase inutilement la mémoire de travail vitale (la fenêtre de contexte limitée) du modèle en plein cours de raisonnement.49 Par conséquent, la totalité des API de recherche interne du système, qu'elles concernent la recherche lexicale, l'énumération de fichiers, ou le requêtage de bases de données, doivent appliquer des limites strictes d'implémentation (par exemple, forcer et tronquer la réponse après la restitution d'un maximum absolu de ![][image7] résultats pertinents, tout en imposant une gestion algorithmique sévère de la pagination pour naviguer dans l'excédent).46 De surcroît, le système global se doit de privilégier nativement le renvoi vers des "Endpoints de Résumé" natifs qui ont la capacité de condenser analytiquement les vastes séries de statistiques volumineuses avant même que l'agent IA désemparé ne soit factuellement forcé de les ingérer dans sa mémoire à court terme.49 Dans un contexte d'exécution d'outils, la mise en œuvre incontournable de "listes d'autorisations" restrictives (allowlists) pour les appels d'outils et de limitations drastiques concernant les droits d'accès au réseau ou aux disques racine ("Restrict file/network access") garantira ultimement que l'IA ne dérive jamais vers des comportements d'exécution de code arbitraire destructeurs (effacement par inadvertance du projet système ou exfiltration involontaire de configurations d'entreprise secrètes).45

L'aperçu de niveau terminal de la dynamique logicielle régissant un tel projet dévoile une vérité d'ingénierie profonde : la construction de l'IDE devient la conception d'un écosystème hautement surveillé, un "bac à sable" interactif opérant en boucle fermée. Dans ce modèle final asynchrone, la véritable et accablante complexité de développement et d'ingénierie du logiciel global ne réside paradoxalement plus du tout dans le composant de base de l'éditeur de texte visuel – les arcanes de la modélisation des Cordes (Ropes) et les spécifications physiques de la canalisation du rendu mathématique via le GPU étant désormais devenues des théories algorithmiquement et technologiquement résolues. La véritable difficulté technique, la "meilleure piste" fondamentale de conception à maîtriser scrupuleusement, réside dans la construction de cet espace virtuel d'interfaçage sécurisé, contextuellement contraint, et extrêmement réactif, espace au sein duquel un agent autonome, de prime abord "aveugle", peut méthodiquement et en toute sécurité développer un modèle mental topographique précis de la complexe base de code sur laquelle il travaille, par tâtonnement itératif, exploration vectorielle sémantique et utilisation méthodique de protocoles d'orchestration avancés.

#### **Works cited**

1. Leveraging Rust and the GPU to render user interfaces at 120 FPS ..., accessed February 14, 2026, [https://zed.dev/blog/videogame](https://zed.dev/blog/videogame)  
2. How Cursor works – Deep dive into vibe coding \- BitPeak, accessed February 14, 2026, [https://bitpeak.com/how-cursor-works-deep-dive-into-vibe-coding/](https://bitpeak.com/how-cursor-works-deep-dive-into-vibe-coding/)  
3. Context | Cursor Learn, accessed February 14, 2026, [https://cursor.com/learn/context](https://cursor.com/learn/context)  
4. Dynamic context discovery · Cursor, accessed February 14, 2026, [https://cursor.com/blog/dynamic-context-discovery](https://cursor.com/blog/dynamic-context-discovery)  
5. Text Editor: Data Structures \- Hacker News, accessed February 14, 2026, [https://news.ycombinator.com/item?id=38772754](https://news.ycombinator.com/item?id=38772754)  
6. Text Editor Data Structures \- invoke::thought(), accessed February 14, 2026, [https://cdacamar.github.io/data%20structures/algorithms/benchmarking/text%20editors/c++/editor-data-structures/](https://cdacamar.github.io/data%20structures/algorithms/benchmarking/text%20editors/c++/editor-data-structures/)  
7. Rope & SumTree — Zed's Blog, accessed February 14, 2026, [https://zed.dev/blog/zed-decoded-rope-sumtree](https://zed.dev/blog/zed-decoded-rope-sumtree)  
8. Making a text editor, should I use a rope, piece table or gap buffer? : r/rust \- Reddit, accessed February 14, 2026, [https://www.reddit.com/r/rust/comments/177jewf/making\_a\_text\_editor\_should\_i\_use\_a\_rope\_piece/](https://www.reddit.com/r/rust/comments/177jewf/making_a_text_editor_should_i_use_a_rope_piece/)  
9. Zed editor switching graphics lib from blade to wgpu \- Hacker News, accessed February 14, 2026, [https://news.ycombinator.com/item?id=47002825](https://news.ycombinator.com/item?id=47002825)  
10. qt \- c++ custom UI ToolKit \-- Options for cross platform abstraction ..., accessed February 14, 2026, [https://stackoverflow.com/questions/14430676/c-custom-ui-toolkit-options-for-cross-platform-abstraction-layer](https://stackoverflow.com/questions/14430676/c-custom-ui-toolkit-options-for-cross-platform-abstraction-layer)  
11. What's the GUI sublimetext uses? \- General Discussion \- Sublime Forum, accessed February 14, 2026, [https://forum.sublimetext.com/t/whats-the-gui-sublimetext-uses/14102](https://forum.sublimetext.com/t/whats-the-gui-sublimetext-uses/14102)  
12. GPUI: UI Framework from the makers of Zed : r/rust \- Reddit, accessed February 14, 2026, [https://www.reddit.com/r/rust/comments/19fle6w/gpui\_ui\_framework\_from\_the\_makers\_of\_zed/](https://www.reddit.com/r/rust/comments/19fle6w/gpui_ui_framework_from_the_makers_of_zed/)  
13. Think-Time UX: Design to Support Cognitive Latency, accessed February 14, 2026, [https://www.uxtigers.com/post/think-time-ux](https://www.uxtigers.com/post/think-time-ux)  
14. Semantic Code Indexing with AST and Tree-sitter for AI Agents (Part — 1 of 3\) \- Medium, accessed February 14, 2026, [https://medium.com/@email2dineshkuppan/semantic-code-indexing-with-ast-and-tree-sitter-for-ai-agents-part-1-of-3-eb5237ba687a](https://medium.com/@email2dineshkuppan/semantic-code-indexing-with-ast-and-tree-sitter-for-ai-agents-part-1-of-3-eb5237ba687a)  
15. Building RAG on codebases: Part 1 \- LanceDB, accessed February 14, 2026, [https://lancedb.com/blog/building-rag-on-codebases-part-1/](https://lancedb.com/blog/building-rag-on-codebases-part-1/)  
16. Linting code for LLMs with tree-sitter | aider, accessed February 14, 2026, [https://aider.chat/2024/05/22/linting.html](https://aider.chat/2024/05/22/linting.html)  
17. How I use LLMs | Karan Sharma, accessed February 14, 2026, [https://mrkaran.dev/posts/using-llm/](https://mrkaran.dev/posts/using-llm/)  
18. senior: Instead of bothering a senior engineer for suggestions on how to improve your code, ask \`senior\` instead\! powered by \`tree-sitter\`and LLMs. : r/rust \- Reddit, accessed February 14, 2026, [https://www.reddit.com/r/rust/comments/15jmmko/senior\_instead\_of\_bothering\_a\_senior\_engineer\_for/](https://www.reddit.com/r/rust/comments/15jmmko/senior_instead_of_bothering_a_senior_engineer_for/)  
19. Repository context for LLM assisted code completion | Tabby AI coding assistant, accessed February 14, 2026, [https://www.tabbyml.com/blog/repository-context-for-code-completion](https://www.tabbyml.com/blog/repository-context-for-code-completion)  
20. Show HN: CodeRLM – Tree-sitter-backed code indexing for LLM agents | Hacker News, accessed February 14, 2026, [https://news.ycombinator.com/item?id=46974515](https://news.ycombinator.com/item?id=46974515)  
21. Build Real-Time Codebase Indexing for AI Code Generation \- CocoIndex, accessed February 14, 2026, [https://cocoindex.io/blogs/index-code-base-for-rag](https://cocoindex.io/blogs/index-code-base-for-rag)  
22. Step by step approach to RAG my Codebase (Part 2\) | by Osman Mehmood | Medium, accessed February 14, 2026, [https://medium.com/@osman.mehmood2/step-by-step-approach-to-rag-my-codebase-part-2-0b8697b284fd](https://medium.com/@osman.mehmood2/step-by-step-approach-to-rag-my-codebase-part-2-0b8697b284fd)  
23. How Cursor Actually Indexes Your Codebase \- Towards Data Science, accessed February 14, 2026, [https://towardsdatascience.com/how-cursor-actually-indexes-your-codebase/](https://towardsdatascience.com/how-cursor-actually-indexes-your-codebase/)  
24. Using Language Engineering to Build a Smarter RAG for Code \- Strumenta, accessed February 14, 2026, [https://tomassetti.me/using-language-engineering-to-build-a-smarter-rag-for-code/](https://tomassetti.me/using-language-engineering-to-build-a-smarter-rag-for-code/)  
25. cAST: Enhancing Code Retrieval-Augmented Generation with Structural Chunking via Abstract Syntax Tree \- arXiv, accessed February 14, 2026, [https://arxiv.org/html/2506.15655v1](https://arxiv.org/html/2506.15655v1)  
26. Cursor AI-IDE Functional Overview and Implementation Guide | by Saud Ahmad Khan, accessed February 14, 2026, [https://medium.com/@saudkhan1508/cursor-ai-ide-functional-overview-and-implementation-guide-329785f7db76](https://medium.com/@saudkhan1508/cursor-ai-ide-functional-overview-and-implementation-guide-329785f7db76)  
27. Build codebase indexing for RAG and semantic search with live update \- YouTube, accessed February 14, 2026, [https://www.youtube.com/watch?v=G3WstvhHO24](https://www.youtube.com/watch?v=G3WstvhHO24)  
28. Model Context Protocol \- OpenAI for developers, accessed February 14, 2026, [https://developers.openai.com/codex/mcp/](https://developers.openai.com/codex/mcp/)  
29. Introducing the Model Context Protocol \- Anthropic, accessed February 14, 2026, [https://www.anthropic.com/news/model-context-protocol](https://www.anthropic.com/news/model-context-protocol)  
30. Unlocking AWS Knowledge with MCP: A Complete Guide to Model Context Protocol and the MCPraxis…, accessed February 14, 2026, [https://ashishkasaudhan.medium.com/unlocking-aws-knowledge-with-mcp-a-complete-guide-to-model-context-protocol-and-the-mcpraxis-597663eb451c](https://ashishkasaudhan.medium.com/unlocking-aws-knowledge-with-mcp-a-complete-guide-to-model-context-protocol-and-the-mcpraxis-597663eb451c)  
31. Stop Hard-Coding AI Tools: The 2026 Guide to Model Context Protocol (MCP) | by Kapil Khatik | Jan, 2026, accessed February 14, 2026, [https://medium.com/@kapildevkhatik2/stop-hard-coding-ai-tools-the-2026-guide-to-model-context-protocol-mcp-5d25fabff608](https://medium.com/@kapildevkhatik2/stop-hard-coding-ai-tools-the-2026-guide-to-model-context-protocol-mcp-5d25fabff608)  
32. Specification \- Model Context Protocol, accessed February 14, 2026, [https://modelcontextprotocol.io/specification/2025-11-25](https://modelcontextprotocol.io/specification/2025-11-25)  
33. How to Build a stdio MCP Server in Rust \- Shuttle.dev, accessed February 14, 2026, [https://www.shuttle.dev/blog/2025/07/18/how-to-build-a-stdio-mcp-server-in-rust](https://www.shuttle.dev/blog/2025/07/18/how-to-build-a-stdio-mcp-server-in-rust)  
34. Build an MCP server \- Model Context Protocol, accessed February 14, 2026, [https://modelcontextprotocol.io/docs/develop/build-server](https://modelcontextprotocol.io/docs/develop/build-server)  
35. MCP in Rust: A Practical Guide using \- HackMD, accessed February 14, 2026, [https://hackmd.io/@Hamze/SytKkZP01l](https://hackmd.io/@Hamze/SytKkZP01l)  
36. Building an AI-native engineering team | OpenAI, accessed February 14, 2026, [https://cdn.openai.com/business-guides-and-resources/building-an-ai-native-engineering-team.pdf](https://cdn.openai.com/business-guides-and-resources/building-an-ai-native-engineering-team.pdf)  
37. Building an AI-Native Engineering Team \- OpenAI for developers, accessed February 14, 2026, [https://developers.openai.com/codex/guides/build-ai-native-engineering-team/](https://developers.openai.com/codex/guides/build-ai-native-engineering-team/)  
38. MSDN Magazine: Task-Based Programming \- Scalable Multithreaded Programming with Tasks | Microsoft Learn, accessed February 14, 2026, [https://learn.microsoft.com/en-us/archive/msdn-magazine/2010/november/msdn-magazine-task-based-programming-scalable-multithreaded-programming-with-tasks](https://learn.microsoft.com/en-us/archive/msdn-magazine/2010/november/msdn-magazine-task-based-programming-scalable-multithreaded-programming-with-tasks)  
39. Better performance through threading | App quality \- Android Developers, accessed February 14, 2026, [https://developer.android.com/topic/performance/threads](https://developer.android.com/topic/performance/threads)  
40. Which is the most recommended thread pattern for game AI and how to implement it? \[closed\] \- Stack Overflow, accessed February 14, 2026, [https://stackoverflow.com/questions/8554643/which-is-the-most-recommended-thread-pattern-for-game-ai-and-how-to-implement-it](https://stackoverflow.com/questions/8554643/which-is-the-most-recommended-thread-pattern-for-game-ai-and-how-to-implement-it)  
41. Built an MCP Client into my Rust LLM inference engine \- Connect to external tools automatically\! \- Reddit, accessed February 14, 2026, [https://www.reddit.com/r/rust/comments/1l8aonx/built\_an\_mcp\_client\_into\_my\_rust\_llm\_inference/](https://www.reddit.com/r/rust/comments/1l8aonx/built_an_mcp_client_into_my_rust_llm_inference/)  
42. Cursor: The best way to code with AI, accessed February 14, 2026, [https://cursor.com/](https://cursor.com/)  
43. OpenAI Codex App: A Guide to Multi-Agent AI Coding | IntuitionLabs, accessed February 14, 2026, [https://intuitionlabs.ai/articles/openai-codex-app-ai-coding-agents](https://intuitionlabs.ai/articles/openai-codex-app-ai-coding-agents)  
44. Building an Agentic Coding Editor — High‑Level Architecture & Infrastructure \- Medium, accessed February 14, 2026, [https://medium.com/@testth02/building-an-agentic-coding-editor-high-level-architecture-infrastructure-3ec173c39aa9](https://medium.com/@testth02/building-an-agentic-coding-editor-high-level-architecture-infrastructure-3ec173c39aa9)  
45. AI skeptic, went “all in” on an agentic workflow to see what the hype is all about. A review, accessed February 14, 2026, [https://www.reddit.com/r/ExperiencedDevs/comments/1lz4dmj/ai\_skeptic\_went\_all\_in\_on\_an\_agentic\_workflow\_to/](https://www.reddit.com/r/ExperiencedDevs/comments/1lz4dmj/ai_skeptic_went_all_in_on_an_agentic_workflow_to/)  
46. Designing an API for AI agents to consume? | by Debraj Maity | Feb, 2026 \- Medium, accessed February 14, 2026, [https://debrajmaity.medium.com/designing-an-api-for-ai-agents-to-consume-c1590aaf59e9](https://debrajmaity.medium.com/designing-an-api-for-ai-agents-to-consume-c1590aaf59e9)  
47. How To Prepare Your API for AI Agents \- The New Stack, accessed February 14, 2026, [https://thenewstack.io/how-to-prepare-your-api-for-ai-agents/](https://thenewstack.io/how-to-prepare-your-api-for-ai-agents/)  
48. Rethinking API Design for Agentic AI \- MuleSoft Blog, accessed February 14, 2026, [https://blogs.mulesoft.com/automation/api-design-for-agentic-ai/](https://blogs.mulesoft.com/automation/api-design-for-agentic-ai/)  
49. Tips for Designing APIs for AI Agents? : r/AI\_Agents \- Reddit, accessed February 14, 2026, [https://www.reddit.com/r/AI\_Agents/comments/1i16hod/tips\_for\_designing\_apis\_for\_ai\_agents/](https://www.reddit.com/r/AI_Agents/comments/1i16hod/tips_for_designing_apis_for_ai_agents/)  
50. How to Build an MCP Server in Rust \- OneUptime, accessed February 14, 2026, [https://oneuptime.com/blog/post/2026-01-07-rust-mcp-server/view](https://oneuptime.com/blog/post/2026-01-07-rust-mcp-server/view)  
51. A practical guide to the architectures of agentic applications | Speakeasy, accessed February 14, 2026, [https://www.speakeasy.com/mcp/using-mcp/ai-agents/architecture-patterns](https://www.speakeasy.com/mcp/using-mcp/ai-agents/architecture-patterns)

[image1]: <data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACUAAAAWCAYAAABHcFUAAAACD0lEQVR4Xu2Vyyt9URTHl1DUT0h5ZERKouSdgYkoBgwwM2FkwsRAmZmYS0zMjI29yojJL/4AEikpRVKKgfJYX2ufe9ZZZ1/3chXJp76ds7/7cfZj7XWI/villLB6VbmVNcKqU56PHdaKNdMkl6S/l3zWI2uDVcFaZr0otYVNIxSSDIrnZ9m1BrhnHbr3YZJG5a48xnoimdiS8zTom2XND5JjjVtWu3vHxPBxSwOJ/2D8Tta68TKmlMJJYLV43w+rExRQeIyaOdaE8TKiinXBmnRlBHNtWB3D7lQj645VpryAJtYea8qVEa//Weck4ZHt/BiYDD7UYis84MzR9lJ5A877p7yAM9Y4SX0z64jVw+py3kLYNAp2CQ3ybIUH7CLazilv0XmWagoXinrsjAbejfES4Nb4BvUxSnIhEPABq+Tv30eSHrBY1CMPaeCdGi/BM0mDYlthGCJpW2P8ZJMK6CdZuAa7iD5JL0ewU6li6pgkT1lSTWqedWA8TAZxicl5QSLEoHY1mjXWDPlvCz6A/rHERxKDiBscewAWj2/heEGQrCPUs65JBh6kaFbGezfJbydZtsZtwrEih1kwHsbV/8xp51W68paqi3FC0lhrk9WhG3lAICObY3KWbYofLYIf415RNLV8OUjA3mP4buyO/AjwM8dlSBZ76TBrja8AFyZVDL7HexkgI4qskSZv/85XTj1yyt8v9k4AAAAASUVORK5CYII=>

[image2]: <data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAD4AAAAWCAYAAACYPi8fAAADFUlEQVR4Xu2YS6hOURTHl1CeIeUxMCCSKG+SmSglBh4TJmYmRhQDkzsxMLtJkaQYySMDeWZww0AMTIREHikDIYWiPNbPOuucdfY5h+9eXznl/urft8/a++yz1957rb3vFRlkkP+eiao1WXm4aolqs2pO3qKe66qjqVEZnRrayEjVV9Ul1VTVN9WPoKVF0xLjxBznF06pPoq905fZWssn1cOsvEl1I9Rtl2ISDgU7LBZ7d0hih9Y7/l61LCvjPANOmSdm/5zYv6guJjan1Y5PksJRVo3ynaI6Z6wUWz7C847E5rTW8emqV6qd2TMJbHZRXSFd8fmqp6rJwRapc5xkx/d8EveUq3OOiPV9RbVOdVqs/bnYKLBQ9UgsT8FW1e1MQ72R4wMgTv/EMLG2r4NtvZhjY4ItUuf4y0zjxd6jvKDUQmS36p5Y/8fF+tmnWql6Edo5OP1MdVLVq1qUlVeL9YOtlINYbTodEY0NsBto2xNsB1XHwnNKdBwnr4klxzgID68L2TNj4TlO5l7VW2k+Uh+ILd5+sSRMcnaYvMq7ZGM+0gnbxJIgSc45ITaoJqLjzP531ca8toB21AFhU+c4trXBFrkldpSSZAkJ7h8O71bCkY/R4YRorIHB0nZmYu+P47TlmRVIwR4XgFWblZVxAmcOi4Xb76CPNGwJzUry9RVPG6c8FhtMSn8cJyQ6dZxtn8b4lFDfRN0iYpuR2H7FGxVMQB1snzNimbeSGcVm8qw0r0R03E8Q4jCFdtQ5d0O5UzwHRVhQDyG+v8Ir5qreiL2wQapJ54nYFbbuVgbE7U2xM76O6Dh9MIH3xe4OzjSxdmRyh8FyZUZNJ0YK4yeJRXapnmdldqYfdTk46NvNdVmaHXbIwAySCYj0SbmvuKM4k7kdcpdHlNPvnJfqeFwxcTmjVFelfOIAO5Z3iPNVSd1fwxb1O3436JHiQhVhcg5Ifaj8E/yo6RYkQY7OOsgprXEc+AOH+E237ECgjw9ity3uDMQ5v+Qa7N34RlchSS5PjQOEJMQd/Z3YbuJ3i7T4nxrcv1vNTzD90Jxe73FXAAAAAElFTkSuQmCC>

[image3]: <data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACQAAAATCAYAAAD4f6+NAAABz0lEQVR4Xu2WvS8EURTFr6AQEvGRiMpHI0LnqxWJhIICFY0/QO2rE51EI6oNhVotFIqNCr1IhEIiUYiIgkIinJM3b/P27OzMiI3KLznZmfPu7Lvz3n131+yfv6cK2oZqdCCBA2hMTaUFGo+ua6FBaA7qLUTEswx1qJnCMPSopocTXkPzUDX0au4N9qGvSE9Qn38gYARaUzNgFZpSM4IvfacmeTOXEJmFzoKxRejTXFK7gU8aoDzUJn4ztAVdmHuuXEKEcxfxYm75CJPiFyj95vx38c+hZ/GUtIQm/AULcRO6jO67oA9oxgcEMNZvXQjv18VT0hIqHAQm8AAtRfesox4/GIOuUH3kTQZeHGkJFTgyF9ykAzEwhrH5wBuA7qH2wIsjc0IsJt2CciyYqzXWkoeT5M0VdhKZE/I1waVPog46gfasuPFVPCG/Qlz6JG7MHXul4gmxpzC4pA9ENEKH5rowm6XCQ8BO260DAudYUTMOdl12Xz4wbe5oe3h9a64NhH5Iq7lTN6QDQuaEPJzY15PXsZVPJISxO2qaO3n6nX43tERKtpzbsQHlos/RotFk2KWv1Pwh4cn9NXxbvvVP/nYo/peiYrD4T9XMSCeU+wZcYGfYlA+3eQAAAABJRU5ErkJggg==>

[image4]: <data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAZCAYAAAB3oa15AAACb0lEQVR4Xu2WT4jNURTHj1BE/jRqKDURyp+NhaRkRbKymFlYyY5iTVkpG3aS1aRkodAsKQuLKaVE/tQwpRQSC0kpUvLnfN45t3fcub+Z936/eSb1PvXt/e45v3vuPfeee39PpE+f/4rruaEDtku9fl2zRHVR9UT1WzUafEOql6qFwdYN9KM/cXrGXbGJL/XfL8F3RfUqtOvwXixOT2D1mfRpbw+KJQLLVZNua8I61TvVQO6YDdKqn8odyiGx0mrKPNUlsXizTlUC61UfVGsze12Ixy4Uma96IDaR16plbl8hdiBvqI6pVovV8y33nxOrTfo98ncXue+A2ymxEox5RyzWFrExz6vuqfaE9xIs1K/cCOkAsk3Aqad92dvU3WOxzmdVt92/y/1VO3DV7SUuiI0LO1TfVDelPXZVv6Id4+fMxmQ/hTaTob1ZtUmsrllBqEogJVqCXdzqz3vFxuMX6PPTn3PiQregrlKHt0H5KpDAmGpBsCWqEhh3+0wQl/dKsXN4L91wLVhRjOPRWIAEUImmCbD6lFAnEC+dsRYr3fgwGgvUSeCa20twOaSJ8M7z4Ful2h/akWK8CTFH/NTvlHZQtpZt7jaBo27/q2alXbZcidv8OX0EgXH3hXaCeRQToIw4VHGCL1SHVYtVR1Rv/B1sJJc4qDouFvi+aiT4dqt+iO1yJH0fuGa5iTh/3ECwQXVSpiYN7Ez8izIFdoCrcU3uaAC3FTuRw64xTpoo7Y0y/R++E6ozubHXsCB8rNjJJtCfOOnq/ad8lHaJ1IX+/KWeE4ZVX3Njl3wXizNn8NV+mhs7YEj1TMqHuk+fpvwBYwqQEBOJGNEAAAAASUVORK5CYII=>

[image5]: <data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAD4AAAAZCAYAAABpaJ3KAAACiElEQVR4Xu2WT6hNURTGP6GIEpmIEsmEkiQ9mTGRSP5kwMCMgYEYKJlQhq8k3kAKCQOGRJIuAykTA1Lem5AYCFFMFL7vrb3cvfc59895ckv2r77u2Wets+/ae62z9gEKhcL/xFLqdaSnqRnTqItIfU7EDgNgEnUSaQyKqxPyP4jUf0biQRZQ56i71M+gmOnUAepbsN2nViQef58p1C7qCtoxzk08UtZQX2F+WttIam6j3bhH3YE570nN4xylWvnNAbObOgSLcVVmc5TtU9RV6l1mq7CYGqO2UT+ox6l5nFvU6fzmgNH/b4EtfHNmc1ZSy6jnsJi7oglvhGuVtSZWecW8gm1QP6yGbWIndsI2uimKYV74rUvCfuostQ+2hg2puUoL5iwWwR5a/ttq6FWoNIgufKb25jdhpfiB2pQb+kBZnE09gSUqT84DamGwaQ3zU3MVTRg3i0/UG9gmCGW65+7VoMUdjsa6fh+Nm6AFe3LOo519R81varj+jmqTruUCLBPONdiDKnuxEdb9J8JHanKQrtem5r5RM/OGdoT6gvR0aUXXil2L74oezjukjjA/Nmbiz7u5jhZpoigpSo6zDhaburzYDotZqLxl857VkXjCmBewCYap0czWBJXlkiCdqV6OTdBC30ZjlbhiuwRrZmpqjj50lG090xW933WonDzrPXevA2qGL6PxM9hGNEVntxqaoypUXGq4N8NY6GtOR1jesyqohPJPVEe2M7AvNh1PTblNPcxvwnrFZTQ7IZRtL2vHkxL3Jj/jvQlWkLMmug5z3AE7BnKG0Mfu1TAL9mk7JzcEtCH6715spY7BYlS1rI9snnFHa3gU7h8PY3/vC4VCoVAo/OP8AmSfi+2p8oZkAAAAAElFTkSuQmCC>

[image6]: <data:image/png;base64,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>

[image7]: <data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABIAAAAZCAYAAAA8CX6UAAAA90lEQVR4Xu2SsQ4BQRRFn0IhkWgkajq1iAiVSiVKCb1G5R+0KhE0So1CKSqlD9AhEj+gUmhw3z6bnZndWRS6PclJzN7J7Ly7iCJ+pQYvihM9dlBzdqjHQgnO4Bk+4VWPHabwSJKPYUWPPfLwQHIIb87osUOVZF8oDbiHC5KDWnrs0IdJ86HJFnZhCu7gHZaVPE3yoo/wWIX37w7JrUZeTEV4UtZW5jCmrPkg1mUJB8o6EJ6bx1IxS+exuMdQeAN3oFKHD7iGCZIOvyo6iB7JrVb0xW0YW4lZ8rrKGZmPOEmRNtyurPAXasMNvMEm+Xti3D9oRMTfeAGD6zVGfO/LPQAAAABJRU5ErkJggg==>